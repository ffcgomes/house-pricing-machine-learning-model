{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Implementing Linear Regression with ColumnTransformer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJnfHCnyuILN"
      },
      "source": [
        "#Implementing Linear Regression with ColumnTransformer on Sci-kit Learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHFEdHKeujKh"
      },
      "source": [
        "##Sci-Kit learn has a linear regression module that makes it easy to implement the algorithm on your data. The data I will be using is the California Housing Prices dataset. I gave a brief explanation of the data in my previous blog post you can access it here. The goal is to predict the housing prices of California from the features in the dataset.\n",
        "Read in the data\n",
        "The first step is to read in our data and preprocess it before ingesting into the linear regression model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMQGaf3KvCPf"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDqUOkgXu5xJ",
        "outputId": "255dfa9a-451d-41a1-9afb-fa6a450baed4"
      },
      "source": [
        "final_housing=pd.read_csv('/content/drive/MyDrive/Github/RealStateMachineLearningModels/Implementing Linear Regression with ColumnTransformer/housing.csv')\n",
        "final_housing.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20640 entries, 0 to 20639\n",
            "Data columns (total 10 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   longitude           20640 non-null  float64\n",
            " 1   latitude            20640 non-null  float64\n",
            " 2   housing_median_age  20640 non-null  float64\n",
            " 3   total_rooms         20640 non-null  float64\n",
            " 4   total_bedrooms      20433 non-null  float64\n",
            " 5   population          20640 non-null  float64\n",
            " 6   households          20640 non-null  float64\n",
            " 7   median_income       20640 non-null  float64\n",
            " 8   median_house_value  20640 non-null  float64\n",
            " 9   ocean_proximity     20640 non-null  object \n",
            "dtypes: float64(9), object(1)\n",
            "memory usage: 1.6+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx6wodmywbwq"
      },
      "source": [
        "###From the info above can see that the total_bedrooms column has some missing values. I missing values was filled with the total_bedroom median value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHO_cg-qwa_-"
      },
      "source": [
        "final_housing['total_bedrooms'].fillna(final_housing['total_bedrooms'].median(), inplace = True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT-hxjJUww-A"
      },
      "source": [
        "##Why Column Transformer?\n",
        "###Column transformer is a module in Scikit-Learn that allows us to perform different preprocessing and feature extraction steps on features of different data types. Our dataset, for instance, contains both continuous and categorical features, these two data types require different preprocessing steps. Categorical features require one-encoding, continuous features requires standard scaling, the Column Transformer makes it possible to apply these preprocessing steps to these features. The pipeline module will then be used in integrating the preprocessed features, which will be used in building our Linear Regression model.\n",
        "##Preprocessing our data\n",
        "###The code scripts below will be showing how to use the Column Transformer and Pipeline module on our dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9WizGKtwryh"
      },
      "source": [
        "# Import all needed modules\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbHyAlP-w9yY"
      },
      "source": [
        "##Preprocess the Numeric columns\n",
        "###We apply PolynomialFeatures and StandardScaler preprocessing steps to the numeric columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqTOF1sxw7Hf"
      },
      "source": [
        "numeric_features = ['longitude','latitude','housing_median_age',\n",
        "                    'total_rooms','total_bedrooms',\n",
        "                    'population','households','median_income']\n",
        "numeric_transformer = Pipeline(steps=[('poly',PolynomialFeatures(degree =2)),\n",
        "                                      ('scaler', StandardScaler())])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3dzVNKUxHdh"
      },
      "source": [
        "##Preprocess the Categorical Column\n",
        "###We one-hot encode our categorical data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpDa2azWxDgN"
      },
      "source": [
        "categorical_features = ['ocean_proximity']\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjKh1ZZIxOF9"
      },
      "source": [
        "###Combine the two preprocessed steps together using the Column Transformer module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHzcph1QxMFE"
      },
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXed3s9ExqXU"
      },
      "source": [
        "##Integrate the Preprocessed Features and Linear Regression Model Using Pipelines\n",
        "\n",
        "###The next step is the integrate the features we just preprocessed with our Machine Learning algorithm to enable us to build a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TduQRFTJxRYs"
      },
      "source": [
        "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('classifier', LinearRegression())])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGe2AyYayIMy"
      },
      "source": [
        "##Train our Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKR5yf43x9hg",
        "outputId": "de601bc8-f371-449d-c518-50f4bb7975bd"
      },
      "source": [
        "X = final_housing.drop('median_house_value', axis = 1)\n",
        "y = final_housing['median_house_value']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('preprocessor',\n",
              "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
              "                                   sparse_threshold=0.3,\n",
              "                                   transformer_weights=None,\n",
              "                                   transformers=[('num',\n",
              "                                                  Pipeline(memory=None,\n",
              "                                                           steps=[('poly',\n",
              "                                                                   PolynomialFeatures(degree=2,\n",
              "                                                                                      include_bias=True,\n",
              "                                                                                      interaction_only=False,\n",
              "                                                                                      order='C')),\n",
              "                                                                  ('scaler',\n",
              "                                                                   StandardScaler(copy=True,\n",
              "                                                                                  with_mean=True,\n",
              "                                                                                  with_std=True))],\n",
              "                                                           verbose=...\n",
              "                                                   'population', 'households',\n",
              "                                                   'median_income']),\n",
              "                                                 ('cat',\n",
              "                                                  Pipeline(memory=None,\n",
              "                                                           steps=[('onehot',\n",
              "                                                                   OneHotEncoder(categories='auto',\n",
              "                                                                                 drop=None,\n",
              "                                                                                 dtype=<class 'numpy.float64'>,\n",
              "                                                                                 handle_unknown='ignore',\n",
              "                                                                                 sparse=True))],\n",
              "                                                           verbose=False),\n",
              "                                                  ['ocean_proximity'])],\n",
              "                                   verbose=False)),\n",
              "                ('classifier',\n",
              "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
              "                                  normalize=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnlIwy-byQow"
      },
      "source": [
        "##Evaluate the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "782bE99MyMkC",
        "outputId": "aa5a124d-9ba8-4110-c4f5-3048378ca902"
      },
      "source": [
        "clf.score(X_train, y_train)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7012750467393345"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnK4cz7fyWqx"
      },
      "source": [
        "###Our model gives us 70.13% accuracy.\n",
        "###Let see how our model does on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T84-YeGJyUO2",
        "outputId": "1382ae36-cd71-4561-85ea-5dd283d3e950"
      },
      "source": [
        "clf.score(X_test, y_test)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6954375277548792"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un52VyFGyfB4"
      },
      "source": [
        "###Our model gives us 69.54% on the test set.\n",
        "###From these metrics, we can see that our model is not overfitting, so there’s no need to apply any regularisation techniques to it. Let’s if our model will perform better when we increase the degree of our Polynomial features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKvI472ZydGy"
      },
      "source": [
        "numeric_features = ['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households','median_income']\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[('poly',PolynomialFeatures(degree =3)),\n",
        "                                      ('scaler', StandardScaler())])\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w0ujWgPzRGP"
      },
      "source": [
        "###This is the only preprocessing step that is changed every other step remains the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xFWHroZylOQ",
        "outputId": "90321d34-b0e8-4068-a3e4-569b1f705d7f"
      },
      "source": [
        "clf.score(X_train, y_train)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7012750467393345"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01PLjV8azUxp",
        "outputId": "6817f868-2625-45b1-8119-028f0b7bd25c"
      },
      "source": [
        "clf.score(X_test, y_test)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6954375277548792"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ_83kC-zdHK"
      },
      "source": [
        "###Our train score improves to 74.3% but our test score drops to 67.3%, which is an indication that our model is overfitting our training data. Hence Polynomial features at degree = 2, performs best on both train and test data.\n",
        "\n",
        "###Linear regression with polynomial features of degree 2 gives us the best accuracy score of roughly 70% on both the train and test datasets. In the subsequent blog posts of these series, other algorithms will be tried and hopefully, we can achieve higher accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I1LS3VOzW7M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}